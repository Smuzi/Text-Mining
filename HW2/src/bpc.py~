import sys
import numpy as np
import random
from sklearn.metrics.pairwise import pairwise_distances
from sklearn.preprocessing import normalize
from scipy.sparse import csr_matrix,lil_matrix
import math


def get_new_centroids(X,closest_cluster):
	num_of_clusters = np.amax(closest_cluster)+1
	#lil_matrix fast for calculating mean
	temp_matrix = lil_matrix((num_of_clusters,X.shape[1]))			#num_of..x X.cols sparse matrix
	for i in range(num_of_clusters):
		relevant_cluster = X[closest_cluster == i]
		if relevant_cluster.shape[0]!=0:
			temp_matrix[i] = relevant_cluster.mean(axis=0)			#calculate new mean of centroids
	centroids = csr_matrix(temp_matrix)								#sparse row matrix
	return centroids
	
def random_numbers(S,N):
	#generate S different numbers from 0 to N
	random.seed()
	Range = np.arange(S)
	for i in range(0,S):
		num = random.randint(0,N)
		while num in Range:
			num = random.randint(0,N)
		Range[i] = num
	return Range

def kmean(X,k,initial_centroids=None):
	
	if initial_centroids==None:
		#initial random seeds and centroids:
		seeds_number = random_numbers(k,X.shape[0]-1) 					#shape[0] returns number of rows
		initial_centroids = X[seeds_number]
	
	#create clusters:
	distance_matrix = pairwise_distances(X,initial_centroids,metric='cosine')		#calculate distance from each centroid
	closest_cluster_id = distance_matrix.argmin(axis=1) 			#get smallest distance
	#added:
	#reduced_clusters = reduce_clusters(np.asarray(closest_cluster_id).reshape(-1))
	#closest_cluster_id = reduced_clusters
	#print closest_cluster_id!=reduced_clusters
	#endadd
	return kmean_body(X,closest_cluster_id)
	
	

def kmean_body(X,closest_cluster):
	counter = 0
	closest_cluster_prev = closest_cluster
	while True:
		centroids = get_new_centroids(X,closest_cluster_prev)
		distances_matrix = pairwise_distances(X,centroids,metric='cosine')
		closest_cluster_new = distances_matrix.argmin(axis=1)
		#added:
		#closest_cluster_new = reduce_clusters(np.asarray(closest_cluster_new).reshape(-1))
		#endadd
		if counter>=20 or np.count_nonzero(closest_cluster_new != closest_cluster_prev) <= 0.1 * X.shape[0]:
			return closest_cluster_prev
		closest_cluster_prev = closest_cluster_new
		counter +=1
def bipartite_clustering(D2W,word_cluster_num,doc_cluster_num,metric):
	W2D = D2W.transpose()
	W2WC = kmean(W2D,word_cluster_num)
	#word_cluster_num = np.amax(W2WC)+1
	#print "wc:",word_cluster_num
	for loop in range(20):
		#D2WC = D2W.dot(transform_from_index_array(W2WC,W2WC.size,word_cluster_num))
		#print D2WC
		#print loop
		new_centroids = get_new_centroids(W2D,W2WC)
		new_distance_matrix = pairwise_distances(W2D,new_centroids,metric=metric) #how to calculate distance? maybe 1-matrix?
		#print new_distance_matrix
		D2WC = D2W.dot(new_distance_matrix)
		if loop==0:
			D2DC = kmean(D2WC,doc_cluster_num)
		else:
			new_centroids = get_new_centroids(D2WC,D2DC)
			D2DC = kmean(D2WC,doc_cluster_num,new_centroids)
		#doc_cluster_num = np.amax(D2DC)+1
		#print "dc:",doc_cluster_num
		new_centroids = get_new_centroids(D2W,D2DC)
		new_distance_matrix = pairwise_distances(D2W,new_centroids,metric=metric) #how to calculate distance? maybe 1-matrix?
		
		W2DC = W2D.dot(new_distance_matrix)
		new_centroids = get_new_centroids(W2DC,W2WC)
		W2WC = kmean(W2DC,word_cluster_num,new_centroids)
		#word_cluster_num = np.amax(W2WC)+1
		#print "wc:",word_cluster_num 
	return D2DC,W2WC

def get_df(wordDF):
	with open(wordDF) as f:
		words_df = f.readlines()
	array_df=[]
	for word in words_df:
		temp = word.strip().split(":")
		#array_df[temp[0]]=temp[1]
		array_df.append(temp[1])
	return np.asarray(array_df)

def main(wc,dc,filename,docOutput,wordOutput,wordDF):
	metric = 'cosine'
	num_WC = wc
	num_DC = dc
	words_df = get_df(wordDF)
	with open(filename) as f:
		documents = f.readlines()	
	rows = []
	cols = []
	data = []
	wordcount = words_df.shape[0]
	for document_id,document in enumerate(documents):
		for pair in document.strip().split(" "):
			pair_array = pair.split(":")
			word_id = pair_array[0]
			word_tf = pair_array[1]
			word_idf = math.log((wordcount/words_df[int(word_id)].astype(int))+1)
			rows.append(document_id)
			cols.append(word_id)
			#data.append(word_tf)
			#data.append(word_idf)
			data.append(int(word_tf)*word_idf)
	number_of_words = np.amax(np.array(cols).astype(int)) + 1
	number_of_documents = document_id + 1
	
	Doc2Word = csr_matrix((np.array(data).astype(float),(np.array(rows),np.array(cols))),shape=(number_of_documents,number_of_words),dtype=float)
	Word2Doc = Doc2Word.transpose(copy=True)
	Doc2DocC,Word2WordC = bipartite_clustering(Doc2Word,num_DC,num_WC,metric)
	#Word2WordC,Doc2DocC = bipartite_clustering(Word2Doc,num_WC,num_DC,metric)

	DC_out = ['%i %i\n' % (doc_id, cluster_id) for doc_id, cluster_id in enumerate(Doc2DocC)]
	WC_out = ['%i %i\n' % (doc_id, cluster_id) for doc_id, cluster_id in enumerate(Word2WordC)]
	
	with open(docOutput,'w') as f:
		f.writelines(DC_out)
	with open(wordOutput,'w') as f:
		f.writelines(WC_out)
	
	output = {}
	output['word_sum_cosine'] = normalize(Word2Doc).multiply(normalize(get_new_centroids(Word2Doc,Word2WordC))[Word2WordC]).sum()
	output['doc_sum_cosine'] = normalize(Doc2Word).multiply(normalize(get_new_centroids(Doc2Word,Doc2DocC))[Doc2DocC]).sum()
	#output['doc_F1'] = eval_macro_f1(DC_out,"C:\Smuzi\CMU\ML Txt mining\HW\HW2\data\HW2_dev.gold_standards")
	print output
	
	return '0'
