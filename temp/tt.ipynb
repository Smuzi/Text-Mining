{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Informtion Processing Wet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 1: \n",
    "\n",
    "# A function which creates a range of x values, in order to compare the true and empiric CDF\n",
    "def create_x_range(x,y,jump):\n",
    "    frange = []\n",
    "    current_x = x\n",
    "    while current_x < y :\n",
    "        frange += [current_x]\n",
    "        current_x += jump\n",
    "    return frange\n",
    "\n",
    "# The true normal CDF\n",
    "from scipy.stats import norm\n",
    "def cal_CDF(x_values):\n",
    "    CDF = []\n",
    "    for x in x_values:\n",
    "        CDF.append(norm.cdf(x))\n",
    "    return CDF\n",
    "\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "num_samples = 100\n",
    "\n",
    "num_CDF_outside_band = 0  # The number of times that the true CDF is outside the confidence band\n",
    "# The best and worst empirical CDF in matter of max(empiric_CDF(x) - true_CDF(x)):\n",
    "best_empirical_CDF = [] \n",
    "global_best_CDF_diff = 0\n",
    "worst_empirical_CDF = []\n",
    "global_worst_CDF_diff = 0\n",
    "\n",
    "# calculate true CDF\n",
    "x_values = create_x_range(-2,2,0.01)\n",
    "true_CDF = cal_CDF(x_values) \n",
    "\n",
    "for j in range(1,1000):\n",
    "    # generate 100 samples\n",
    "    samples = np.random.normal(mu, sigma, num_samples)\n",
    "    \n",
    "    # calculate confidence band\n",
    "    sample_sigma = np.var(samples, ddof=1)\n",
    "    alpha = 0.05\n",
    "    confidence_interval = math.sqrt((math.log(2/alpha))/float(2*num_samples))\n",
    "    \n",
    "    # calculate empirical CDF\n",
    "    empirical_CDF = []\n",
    "    empiric_val = 0\n",
    "    i = 0\n",
    "    for x in x_values:\n",
    "        empirical_CDF.append(0)\n",
    "        for sample in samples:\n",
    "            if (sample < x):\n",
    "                empirical_CDF[i] += 1\n",
    "        empirical_CDF[i] = empirical_CDF[i] / float(num_samples)\n",
    "        i += 1\n",
    "    \n",
    "    # calculate whether the true CDF is inside the cofidence band, and the best and worst empirical_CDF\n",
    "    max_CDF_diff = 0\n",
    "    i = 0\n",
    "    for x in x_values:\n",
    "        diff = abs(empirical_CDF[i] - true_CDF[i])\n",
    "        max_CDF_diff = max(max_CDF_diff, diff)\n",
    "        i += 1\n",
    "    \n",
    "    if (max_CDF_diff > confidence_interval):\n",
    "        num_CDF_outside_band += 1\n",
    "    \n",
    "    if (max_CDF_diff > global_worst_CDF_diff):\n",
    "        global_worst_CDF_diff = max_CDF_diff\n",
    "        worst_empirical_CDF = empirical_CDF[:]\n",
    "        \n",
    "    if ((max_CDF_diff < global_best_CDF_diff) or (global_best_CDF_diff == 0)):\n",
    "        global_best_CDF_diff = max_CDF_diff\n",
    "        best_empirical_CDF = empirical_CDF[:]\n",
    "\n",
    "# print results:\n",
    "# precentage of CDF outside confidence band:\n",
    "CDF_outside_band_percentage = num_CDF_outside_band / float(1000)\n",
    "print ((1-CDF_outside_band_percentage), \"percentage of time the interval contained the true CDF\")\n",
    "\n",
    "# plot the true, best and worst CDF:\n",
    "true, = plt.plot(x_values, true_CDF, 'b', label=\"true CDF\")\n",
    "best, = plt.plot(x_values, best_empirical_CDF, 'r', label=\"best empiric CDF\")\n",
    "worst, = plt.plot(x_values, worst_empirical_CDF, 'g', label=\"worst empiric CDF\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('CDF(x)')\n",
    "plt.xlabel('x values')\n",
    "plt.title(\"CDF comparison\")\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Question 2: \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samsungData = pd.read_csv('samsungData.csv')\n",
    "samsungData = samsungData.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "\n",
    "# Question 3:\n",
    "num_examples, num_features = samsungData.shape\n",
    "\n",
    "#calculate the correlation between each pair of features:\n",
    "corr_df = samsungData.corr()\n",
    "corr_df.to_csv('corr_df.csv')\n",
    "\n",
    "# present the data as heatmap:\n",
    "plt.pcolor(corr_df)\n",
    "plt.yticks(np.arange(0.5, len(corr_df.index), 1))\n",
    "plt.xticks(np.arange(0.5, len(corr_df.columns), 1))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Question 4:\n",
    "# find the two most correlated features:\n",
    "# sort the correlation dataframe, and exclude cells which represent the same feature (which means: variance and not correlation)\n",
    "\n",
    "labels = list(corr_df.index)\n",
    "for i in range(0, (num_features-1)):\n",
    "    corr_df.set_value(labels[i],labels[i],0)\n",
    "sorted_corr_df = corr_df.abs().unstack().sort_values()\n",
    "\n",
    "print \"The two most correlated features are: \", list(sorted_corr_df.index)[-1]\n",
    "print '''One can notice that the most correlated features are those which measure the same aspect,\n",
    "and from that reason their correlation is almost 1'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "samsungData = pd.read_csv('samsungData.csv')\n",
    "samsungData = samsungData.drop(['Unnamed: 0'], axis=1)\n",
    "num_examples, num_features = samsungData.shape\n",
    "\n",
    "# Question 5:\n",
    "# group the features by class:\n",
    "# calculate the correlation between each pair of features per class:\n",
    "corr_by_class = samsungData.groupby('activity').corr()\n",
    "corr_by_class.to_csv('corr_by_class.csv')\n",
    "\n",
    "# present the data as heatmap:\n",
    "plt.pcolor(corr_by_class)\n",
    "plt.yticks(np.arange(0.5, len(corr_by_class.index), 1))\n",
    "plt.xticks(np.arange(0.5, len(corr_by_class.columns), 1))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Question 6:\n",
    "# find the two most correlated features (over all classes):\n",
    "# sort the correlation dataframe, and exclude cells which represent the same feature (which means: variance and not correlation)\n",
    "\n",
    "labels = list(corr_by_class)\n",
    "multiSet = corr_by_class.index\n",
    "classes = list(multiSet.levels)[0]\n",
    "for i in range(0, (num_features-1)):\n",
    "    for cls in classes:\n",
    "        corr_by_class.set_value((cls,labels[i]),labels[i],0)\n",
    "    \n",
    "sorted_corr_by_class = corr_by_class.abs().unstack().unstack().sort_values()\n",
    "\n",
    "print(\"The two most correlated features are: \", list(sorted_corr_by_class.index))[-1]\n",
    "print '''One can notice that the most correlated features are those which measure the same aspect,\n",
    "and from that reason their correlation is almost 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 7:\n",
    "# Use Bootstrap method in order to estimate the variance of the empirical correlation estimators:\n",
    "import pandas as pd\n",
    "\n",
    "samsungData = pd.read_csv('samsungData.csv')\n",
    "samsungData = samsungData.drop(['Unnamed: 0'], axis=1)\n",
    "num_examples, num_features = samsungData.shape\n",
    "\n",
    "# choose two couples of features, upon which the variance will be estimated:\n",
    "x1 = 'tBodyAcc-sma()'\n",
    "y1 = 'tBodyAcc-std()-X'\n",
    "x2 = 'fBodyGyro-energy()-X'\n",
    "y2 = 'tBodyAcc-arCoeff()-Z,4'\n",
    "\n",
    "x1_org_data = samsungData[x1]\n",
    "y1_org_data = samsungData[y1]\n",
    "x2_org_data = samsungData[x2]\n",
    "y2_org_data = samsungData[y2]\n",
    "\n",
    "print '''couples of features chosen: very high correlation: [tBodyAcc-sma(), tBodyAcc-std()-X], \n",
    "        low correlation: [fBodyGyro-energy()-X, tBodyAcc-arCoeff()-Z,4]'''\n",
    "\n",
    "# A function used to sample two vectors (with replacement) and calculate their corraletion:\n",
    "def sample_calc_corr(x_data, y_data):\n",
    "    x_samples = x_data.sample(n=num_examples, replace=True)\n",
    "    y_samples = y_data.sample(n=num_examples, replace=True)\n",
    "    corr = x_samples.corr(y_samples)\n",
    "    return corr\n",
    "\n",
    "# A function used to calculate N times the correlation between two vectors, and the matching variance array:\n",
    "def iterate_calc_var(N, x_data, y_data):\n",
    "    corr_arr = []\n",
    "    var_arr = []\n",
    "    for i in range(1,N):\n",
    "        corr_arr.append(sample_calc_corr(x_data, y_data))\n",
    "        var_arr.append(np.var(corr_arr))\n",
    "    return corr_arr[:], var_arr[:]\n",
    "    \n",
    "\n",
    "# N times: \n",
    "    # Randomly select data (num_examples) for each feature:\n",
    "    # Compute the correlation for each couple of features\n",
    "    # Compute the empirical variance over the N correlations of each couple of features:\n",
    "first_couple_corr_arr = []\n",
    "first_couple_var_arr = []\n",
    "first_couple_iteration = 500  # actually, about 300 iterations are needed, this number will be justified in Question 8\n",
    "second_couple_corr_arr = []\n",
    "second_couple_var_arr = []\n",
    "second_couple_iteration = 800  # actually, about 400 iterations are needed, this number will be justified in Question 8\n",
    "\n",
    "# first couple variance:\n",
    "first_couple_corr_arr, first_couple_var_arr = iterate_calc_var(first_couple_iteration, x1_org_data, y1_org_data)\n",
    "    \n",
    "# second couple variance:\n",
    "second_couple_corr_arr, second_couple_var_arr = iterate_calc_var(second_couple_iteration, x2_org_data, y2_org_data)\n",
    "\n",
    "# print the results:\n",
    "print \"The estimated variance for the correlation estimator: \"\n",
    "print \"couple of feautres with very high correlation, the variance is: \", first_couple_var_arr[-1]\n",
    "print \"couple of feautres with low correlation, the variance is: \", second_couple_var_arr[-1]\n",
    "print '''While viewing those results, one can notice that the correlation estimator has much lower variance when\n",
    "used to estimate a couple of features with a high correlation.'''\n",
    "\n",
    "\n",
    "# Do the same process again for two couples of features given a specific class:\n",
    "\n",
    "# create data tables consisting data only from specific classes:\n",
    "walk_data = samsungData.loc[samsungData['activity'] == 'walk']\n",
    "stand_data = samsungData.loc[samsungData['activity'] == 'standing']\n",
    "sit_data = samsungData.loc[samsungData['activity'] == 'sitting']\n",
    "\n",
    "# estimate the variance per class for each couple of features, as explained above:\n",
    "\n",
    "first_var_arr_walk = []\n",
    "first_var_arr_stand = []\n",
    "first_var_arr_sit = []\n",
    "first_couple_class_iteration = 500  # actually, about 400 iterations are needed, this number will be justified in Question 8\n",
    "\n",
    "second_var_arr_walk = []\n",
    "second_var_arr_stand = []\n",
    "second_var_arr_sit = []\n",
    "second_couple_class_iteration = 1000  # actually, about 600 iterations are needed, this number will be justified in Question 8\n",
    "\n",
    "# first couple variance, by class:\n",
    "_, first_var_arr_walk = iterate_calc_var(first_couple_class_iteration, walk_data[x1], walk_data[y1])\n",
    "_, first_var_arr_stand = iterate_calc_var(first_couple_class_iteration, stand_data[x1], stand_data[y1])\n",
    "_, first_var_arr_sit = iterate_calc_var(first_couple_class_iteration, sit_data[x1], sit_data[y1])\n",
    "\n",
    "# second couple variance, by class:\n",
    "_, second_var_arr_walk = iterate_calc_var(second_couple_class_iteration, walk_data[x2], walk_data[y2])\n",
    "_, second_var_arr_stand = iterate_calc_var(second_couple_class_iteration, stand_data[x2], stand_data[y2])\n",
    "_, second_var_arr_sit = iterate_calc_var(second_couple_class_iteration, sit_data[x2], sit_data[y2])\n",
    "\n",
    "# print the results:\n",
    "var_per_class = pd.DataFrame.from_items([('couples', ['high corr couple', 'low corr couple']),\n",
    "                            ('walk', [first_var_arr_walk[-1], second_var_arr_walk[-1]]),\n",
    "                            ('standing', [first_var_arr_stand[-1], second_var_arr_stand[-1]]),\n",
    "                            ('sitting', [first_var_arr_sit[-1], second_var_arr_sit[-1]])])\n",
    "\n",
    "print \"\" # used only for a better output viewing\n",
    "print \"The estimated variance for the correlation estimator, per class is: \"\n",
    "print var_per_class\n",
    "print '''While viewing those results, one can notice that the correlation estimator has indeed different variance\n",
    "for different classes, but still overall the estimator has much lower variance when\n",
    "used upon high correlation.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 8:\n",
    "# Plot several figures showing converges of the Bootstrap estimator \n",
    "# to justify your choice of iteration number for the Bootstrap method:\n",
    "\n",
    "print \"Below is the variance estimation using an increasing amount of iterations of the bootstrap method:\"\n",
    "plt.figure(1)\n",
    "plt.plot(first_couple_var_arr)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"high correlation couple variance estimation\")\n",
    "\n",
    "print \"It seems that about 300 iterations is enough for the high correlation couple's variance estimation to converge\"\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(second_couple_var_arr)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"low correlation couple variance estimation\")\n",
    "\n",
    "print \"It seems that about 400 iterations is enough for the low correlation couple's variance estimation to converge\"\n",
    "\n",
    "plt.figure(3)\n",
    "plt.plot(first_var_arr_walk, label=\"walk\")\n",
    "plt.plot(first_var_arr_stand, label=\"stand\")\n",
    "plt.plot(first_var_arr_sit, label=\"sit\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"high correlation couple variance estimation, by class\")\n",
    "plt.show()\n",
    "\n",
    "print '''It seems that about 400 iterations is enough for the all of the high correlation couple's classes'\n",
    "variance estimation to converge'''\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(second_var_arr_walk, label=\"walk\")\n",
    "plt.plot(second_var_arr_stand, label=\"stand\")\n",
    "plt.plot(second_var_arr_sit, label=\"sit\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"low correlation couple variance estimation, by class\")\n",
    "plt.show()\n",
    "\n",
    "print '''It seems that about 600 iterations is enough for the all of the low correlation couple's classes'\n",
    "variance estimation to converge'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 9:\n",
    "# Using the variance estimator obtained in the previous section, obtain C.I \n",
    "# with 95% on your estimators:\n",
    "\n",
    "# Assumption: the data is of a Guassian distribtion:\n",
    "def CI(var):\n",
    "    if (type(var) != str):\n",
    "        return (1.96 * math.sqrt(var))  # A 95% C.I calculation when assuming Guassian distribution.\n",
    "    else:\n",
    "        return var\n",
    "\n",
    "# print a C.I table for varirance of the chosen features couples, over all classes:\n",
    "first_CI = CI(first_couple_var_arr[-1])\n",
    "second_CI = CI(second_couple_var_arr[-1])\n",
    "\n",
    "CI_table = pd.DataFrame.from_items([('couples', ['high corr couple', 'low corr couple']),\n",
    "                                    ('CI', [first_CI, second_CI])])\n",
    "                                    \n",
    "print \"Below is the confidence interval table for the chosen features' couples. The CI value represents the CI 'radius'\"\n",
    "print \"\"\n",
    "print CI_table\n",
    "print \"\"\n",
    "\n",
    "\n",
    "# print a C.I table for varirance of the chosen features couples, by class:\n",
    "CI_table_per_class = var_per_class.applymap(CI)\n",
    "                                    \n",
    "print \"Below is the confidence interval table for the chosen features' couples, by class. The CI value represents the CI 'radius'\"\n",
    "print \"\"\n",
    "print CI_table_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 10:\n",
    "# Compute the MLE estimators for the Gaussian distribution per feature in the \"walk\" class:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samsungData = pd.read_csv('samsungData.csv')\n",
    "samsungData = samsungData.drop(['Unnamed: 0'], axis=1)\n",
    "num_examples, num_features = samsungData.shape\n",
    "\n",
    "# fetch the data of the \"walk\" class:\n",
    "walk_data = samsungData.loc[samsungData['activity'] == 'walk']\n",
    "walk_data = walk_data.drop(['activity'],axis=1)\n",
    "\n",
    "# Define functions to calculate mu and sigma, according to the Guassian distribution MLE equations we learned in class:\n",
    "def MLE_mu(data):\n",
    "    return np.mean(data)\n",
    "\n",
    "def MLE_sigma(data):\n",
    "    return math.sqrt(np.var(data))\n",
    "\n",
    "def MLE_column(data):\n",
    "    return pd.Series.from_array([MLE_mu(data), MLE_sigma(data)])\n",
    "\n",
    "# calculate the MLE estimators for each feature:\n",
    "MLE_df = walk_data.apply(MLE_column, raw=True)\n",
    "MLE_df.insert(0,'parameter',['mu', 'sigma'])\n",
    "\n",
    "# save the results on a csv file (too big to print)\n",
    "MLE_df.to_csv('MLE_df.csv')\n",
    "\n",
    "\n",
    "# use the same method from Question 1 in order to figure out which feature is the closest to be a Guassian:\n",
    "\n",
    "x_values = np.arange(-2,2.01,0.01)\n",
    "\n",
    "# The true normal CDF\n",
    "from scipy.stats import norm\n",
    "def cal_specific_CDF(x_values, mu, sigma):\n",
    "    CDF = []\n",
    "    for x in x_values:\n",
    "        CDF.append(norm(mu, sigma).cdf(x))\n",
    "    return CDF\n",
    "\n",
    "# The empiric CDF\n",
    "def empiric_CDF_calc(data):\n",
    "    empirical_CDF = []\n",
    "    empiric_val = 0\n",
    "    i = 0\n",
    "    for x in x_values:\n",
    "        empirical_CDF.append(0)\n",
    "        for sample in data:\n",
    "            if (sample < x):\n",
    "                empirical_CDF[i] += 1\n",
    "        empirical_CDF[i] = empirical_CDF[i] / float(len(data))\n",
    "        i += 1\n",
    "    return empirical_CDF[:]\n",
    "\n",
    "# calc the empiric CDF for each feature (by class \"walk\"):\n",
    "def empiric_CDF_column(data):\n",
    "    return pd.Series.from_array(empiric_CDF_calc(data))\n",
    "empiric_CDF_df = walk_data.apply(empiric_CDF_column, raw=True)\n",
    "#empiric_CDF_df.to_csv('empiric_CDF_df.csv')    \n",
    "    \n",
    "# calc the guassian CDF for each feature (by class \"walk\"):\n",
    "MLE_df_data =  MLE_df.drop(['parameter'],axis=1)\n",
    "def guassian_CDF_column(data):\n",
    "    return pd.Series.from_array(cal_specific_CDF(x_values, *data))\n",
    "\n",
    "#guassian_CDF_df = MLE_df_data.apply(guassian_CDF_column)\n",
    "#guassian_CDF_df.to_csv('guassian_CDF_df.csv') \n",
    "\n",
    "guassian_CDF_df = pd.read_csv('guassian_CDF_df.csv')\n",
    "\n",
    "# The best and worst match to Guassian in matter of max(empiric_CDF(x) - guassian_CDF(x)):\n",
    "best_feature = \"\"\n",
    "global_best_CDF_diff = 0\n",
    "worst_feature = \"\"\n",
    "global_worst_CDF_diff = 0\n",
    "\n",
    "for column in MLE_df_data:\n",
    "    max_CDF_diff = 0\n",
    "    i = 0\n",
    "    for x in x_values:\n",
    "        diff = abs(empiric_CDF_df[column][i] - guassian_CDF_df[column][i])\n",
    "        max_CDF_diff = max(max_CDF_diff, diff)\n",
    "        i += 1\n",
    "\n",
    "    if (max_CDF_diff > global_worst_CDF_diff):\n",
    "        global_worst_CDF_diff = max_CDF_diff\n",
    "        worst_feature = column\n",
    "\n",
    "    if ((max_CDF_diff < global_best_CDF_diff) or (global_best_CDF_diff == 0)):\n",
    "        global_best_CDF_diff = max_CDF_diff\n",
    "        best_feature = column\n",
    "\n",
    "# print the results:\n",
    "print '''The feature that best fits the Guassian distribution is: ''', best_feature\n",
    "print '''The feature that worst fits the Guassian distribution is: ''', worst_feature\n",
    "print '''In order to justify our answer, we used the metric that was proposed in question 1, which\n",
    "means: max(empiric_CDF(x) - guassian_CDF(x)). In order to do that, we calculated the empiric CDF for each feature,\n",
    "and also the guassian CDF (by the mu and sigma MLE estimators), and then found the best and worst matches.\n",
    "below is the graphs of the best and worst matches:'''\n",
    "print \"\"\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_values, empiric_CDF_df[best_feature], label=\"empiric CDF\")\n",
    "plt.plot(x_values, guassian_CDF_df[best_feature], label=\"guassian CDF\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('CDF')\n",
    "plt.xlabel('x values')\n",
    "plt.title(\"Best fit for guassian distribution feature\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_values, empiric_CDF_df[worst_feature], label=\"empiric CDF\")\n",
    "plt.plot(x_values, guassian_CDF_df[worst_feature], label=\"guassian CDF\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('CDF')\n",
    "plt.xlabel('x values')\n",
    "plt.title(\"Worst fit for guassian distribution feature\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
